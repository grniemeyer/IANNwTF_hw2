{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPYHvyvDtyXSm5CpHspWB1m",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/grniemeyer/IANNwTF_hw2/blob/main/2ndtrial_hw2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "LUNuWQNqg8YI"
      },
      "outputs": [],
      "source": [
        "import tensorflow_datasets as tfds\n",
        "from tensorflow.keras.layers import Dense\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
        "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(train_ds, test_ds), ds_info = tfds.load ('mnist', split =['train', 'test'], as_supervised = True, with_info = True)\n",
        "#tfds.show_examples(train_ds, ds_info)\n",
        "\n",
        "def prepare_data(ds):\n",
        "    \"\"\"\n",
        "    Preprocess a TensorFlow dataset for training.\n",
        "    \"\"\"\n",
        "    # Reshape images to a flat vector\n",
        "    ds = ds.map(lambda image, label: (tf.reshape(image, (-1, )),label))\n",
        "    # Normalize image data to a range between -1 and 1\n",
        "    ds = ds.map(lambda image, label: ((tf.cast(image, tf.float32)/128.)-1., label))\n",
        "    # Convert labels to one-hot encoding with a depth of 10\n",
        "    ds = ds.map(lambda image, label: (image, tf.one_hot(label, depth=10)))\n",
        "    # Shuffle the dataset with a buffer size of 1024, batch into groups of 4\n",
        "    ds = ds.shuffle(1024).batch(32)\n",
        "    # Pre-load the batch\n",
        "    ds = ds.prefetch(4)\n",
        "    return ds\n",
        "\n",
        "train_dataset = train_ds.apply(prepare_data)\n",
        "test_dataset = test_ds.apply(prepare_data)"
      ],
      "metadata": {
        "id": "RSKVxpS-hI9e"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP_Model(tf.keras.Model):\n",
        "    \"\"\"\n",
        "    Multi-Layer Perceptron for classification.\n",
        "    \"\"\"\n",
        "    def __init__(self, layer_sizes, output_size=10):\n",
        "      super(MLP_Model, self).__init__() # Initialize the parent class\n",
        "      self.mlp_layers = []\n",
        "      # Layer sizes [, , , ] -- list of integers\n",
        "      # Create hidden Dense layers with sigmoid activation\n",
        "      for layer_size in layer_sizes:\n",
        "        new_layer = tf.keras.layers.Dense(layer_size, 'sigmoid')\n",
        "        self.mlp_layers.append(new_layer)\n",
        "      self.output_layer = tf.keras.layers.Dense(output_size, 'softmax')\n",
        "\n",
        "    def call(self, x):\n",
        "        \"\"\"\n",
        "          Forward pass through the MLP model.\n",
        "        \"\"\"\n",
        "        for layer in self.mlp_layers:\n",
        "          x = layer(x) # Pass the input through each hidden layer\n",
        "        y = self.output_layer(x)\n",
        "        return y"
      ],
      "metadata": {
        "id": "iaOFpf3nhKE7"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(n_epochs, model, train_dataset, test_dataset, loss_function, optimizer):\n",
        "  \"\"\"\n",
        "  Train and test a MLP.\n",
        "  \"\"\"\n",
        "  # Lists to store training and testing metrics across epochs\n",
        "  train_losses = []\n",
        "  train_accuracies = []\n",
        "  test_accuracies = []\n",
        "  test_losses = []\n",
        "\n",
        "  for epoch in range(n_epochs):\n",
        "        # Lists to aggregate losses and accuracies for each batch in the training dataset\n",
        "        epoch_loss_agg = []\n",
        "        epoch_accuracy_agg = []\n",
        "        for x, target in train_dataset:\n",
        "            with tf.GradientTape() as tape:\n",
        "                pred = model(x)\n",
        "                loss = loss_function(target, pred)\n",
        "                epoch_loss_agg.append(loss)\n",
        "\n",
        "            # Compute gradients and update model weights\n",
        "            gradients = tape.gradient(loss, model.trainable_variables)\n",
        "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "            accuracy_tr = tf.keras.metrics.Accuracy()\n",
        "            accuracy_tr.update_state(tf.argmax(target, axis=1), tf.argmax(pred, axis=1))\n",
        "            epoch_accuracy_agg.append(accuracy_tr.result().numpy())\n",
        "\n",
        "        train_accuracies.append(tf.reduce_mean(epoch_accuracy_agg))\n",
        "        train_losses.append(tf.reduce_mean(epoch_loss_agg))\n",
        "\n",
        "         # Lists to aggregate losses and accuracies for each batch in the testing dataset\n",
        "        test_accuracy_aggregator = []\n",
        "        test_loss_aggregator = []\n",
        "        for x, target in test_dataset:\n",
        "            pred = model(x)\n",
        "            test_loss = loss_function(target, pred)\n",
        "\n",
        "            accuracy_te = tf.keras.metrics.Accuracy()\n",
        "            accuracy_te.update_state(tf.argmax(target, axis=1), tf.argmax(pred, axis=1))\n",
        "            test_accuracy_aggregator.append(accuracy_te.result().numpy())\n",
        "\n",
        "            test_loss_aggregator.append(test_loss)\n",
        "\n",
        "        test_accuracies.append(tf.reduce_mean(test_accuracy_aggregator))\n",
        "        test_losses.append(tf.reduce_mean(test_loss_aggregator))\n",
        "\n",
        "        print(f'Epoch: {epoch}, Test Accuracy: {test_accuracies[-1]}, Test Loss: {test_losses[-1]}')\n",
        "  return train_losses, train_accuracies, test_losses, test_accuracies"
      ],
      "metadata": {
        "id": "xc8DyW-PhNeZ"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = train_dataset.take(1000)\n",
        "test_dataset = test_dataset.take(1000)\n",
        "\n",
        "model = MLP_Model(layer_sizes = [256, 256])\n",
        "cce = tf.keras.losses.CategoricalCrossentropy() # Define the Categorical Crossentropy loss function\n",
        "optimizer = tf.keras.optimizers.SGD(0.001) # Define the Stochastic Gradient Descent optimizer with a learning rate of 0.001\n",
        "training = train(5, model, train_dataset, test_dataset, cce, optimizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ha2X18wphRqv",
        "outputId": "36dd8071-5908-4042-b70e-7f3b7d9ab893"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Test Accuracy: 0.21106229722499847, Test Loss: 2.260310173034668\n",
            "Epoch: 1, Test Accuracy: 0.3029153347015381, Test Loss: 2.2266860008239746\n",
            "Epoch: 2, Test Accuracy: 0.444888174533844, Test Loss: 2.1909801959991455\n",
            "Epoch: 3, Test Accuracy: 0.5446286201477051, Test Loss: 2.152503728866577\n",
            "Epoch: 4, Test Accuracy: 0.5709864497184753, Test Loss: 2.1096558570861816\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PXjP358khc6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualization(train_losses, train_accuracies, test_losses, test_accuracies):\n",
        "    \"\"\"\n",
        "    Visualizes accuracy and loss for training and test data using\n",
        "    the mean of each epoch.\n",
        "    Loss is displayed in a regular line, accuracy in a dotted line.\n",
        "    Training data is displayed in blue, test data in red.\n",
        "    \"\"\"\n",
        "    # Create a new figure for plotting\n",
        "    plt.figure()\n",
        "    # Plot training and test losses with blue and red lines\n",
        "    line1, = plt.plot(train_losses, \"b-\")\n",
        "    line2, = plt.plot(test_losses, \"r-\")\n",
        "    # Plot training and test accuracies with blue and red dotted lines\n",
        "    line3, = plt.plot(train_accuracies, \"b:\")\n",
        "    line4, = plt.plot(test_accuracies, \"r:\")\n",
        "    # Set the labels for the x and y-axes\n",
        "    plt.xlabel(\"Training steps\")\n",
        "    plt.ylabel(\"Loss / Accuracy\")\n",
        "    # Create a legend for the plot\n",
        "    plt.legend((line1, line2, line3, line4), (\"training loss\", \"test loss\", \"train accuracy\", \"test accuracy\"))\n",
        "    # Display the plot\n",
        "    plt.show()\n",
        "\n",
        "train_losses, train_accuracies, test_losses, test_accuracies = training\n",
        "visualization(train_losses, test_losses, train_accuracies, test_accuracies)"
      ],
      "metadata": {
        "id": "8HM2-VodhUqp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform a grid search\n",
        "layer_sizes_options = [[64, 32], [128, 64, 32], [256, 128, 64]]\n",
        "\n",
        "Adam = tf.keras.optimizers.legacy.Adam(0.001)\n",
        "SGD = tf.keras.optimizers.legacy.SGD(0.001)\n",
        "RMSprop = tf.keras.optimizers.legacy.SGD(0.001)\n",
        "\n",
        "optimizer_options = [Adam, SGD, RMSprop]\n",
        "n_epochs_options = [6, 10, 14]\n",
        "\n",
        "\n",
        "best_params = None\n",
        "best_accuracy = 0.0\n",
        "\n",
        "for layer_sizes in layer_sizes_options:\n",
        "    for optimizer in optimizer_options:\n",
        "        for n_epochs in n_epochs_options:\n",
        "            model = MLP_Model(layer_sizes=layer_sizes)\n",
        "\n",
        "            optimizer_instance = optimizer\n",
        "\n",
        "            train_losses, train_accuracies, test_losses, test_accuracies = train(n_epochs=n_epochs, model=model, train_dataset=train_dataset, test_dataset=test_dataset, loss_function = cce, optimizer=optimizer_instance)\n",
        "\n",
        "            mean_accuracy = np.mean(test_accuracies)"
      ],
      "metadata": {
        "id": "HixmmgwFhYhi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### 4 deviations and their interpretations"
      ],
      "metadata": {
        "id": "edjaTYXdmane"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "adjustable parameters:\n",
        "* learning rate,\n",
        "* batch size,\n",
        "* number and size of layers of model\n",
        "* optimizer (in SGD’s case the momentum hyperparameter)\n",
        "\n",
        "\n",
        "\n",
        "- lr: 0.001, batch size:4, shape: [256, 256], 5 epochs, SGD\n",
        "-- Epoch: 0, Test Accuracy: 0.11124999821186066, Test Loss: 2.2863993644714355\n",
        "-- Epoch: 1, Test Accuracy: 0.2567499876022339, Test Loss: 2.2502074241638184\n",
        "-- Epoch: 2, Test Accuracy: 0.46674999594688416, Test Loss: 2.2150402069091797\n",
        "-- Epoch: 3, Test Accuracy: 0.476500004529953, Test Loss: 2.177403211593628\n",
        "-- Epoch: 4, Test Accuracy: 0.3177500069141388, Test Loss: 2.141585350036621\n",
        "\n",
        "- evth. equal but lr: 0.01\n",
        "-- Epoch: 0, Test Accuracy: 0.49924999475479126, Test Loss: 1.9054900407791138\n",
        "-- Epoch: 1, Test Accuracy: 0.7622500061988831, Test Loss: 1.143931269645691\n",
        "-- Epoch: 2, Test Accuracy: 0.8075000047683716, Test Loss: 0.7798288464546204\n",
        "-- Epoch: 3, Test Accuracy: 0.8364999890327454, Test Loss: 0.6070896983146667\n",
        "-- Epoch: 4, Test Accuracy: 0.8510000109672546, Test Loss: 0.5335938930511475\n",
        "\n",
        "->  with higher lr, NN learned fast espeacially in the beginning, and achieved much higher accuacies in the same amout of epochs. That is because it takes bigger steps in each iteration. However this can also lead to unwanted/unpredictable chnages in the weights as visible in the next example with an even higher lr\n",
        "- evth. equal again except lr: 0.5\n",
        "-- Epoch: 0, Test Accuracy: 0.7537500262260437, Test Loss: 0.7244381308555603\n",
        "-- Epoch: 1, Test Accuracy: 0.8299999833106995, Test Loss: 0.5498851537704468\n",
        "-- Epoch: 2, Test Accuracy: 0.8142499923706055, Test Loss: 0.491791695356369\n",
        "-- Epoch: 3, Test Accuracy: 0.875, Test Loss: 0.4159727096557617\n",
        "-- Epoch: 4, Test Accuracy: 0.7404999732971191, Test Loss: 0.7665658593177795\n",
        "->  here we see that the model learns extremly quickly however the accuracy goes down again after 2 epochs already and it can happen that it just misses the minimum of the loss function... or it can be stuck easily in local minima. Besides it is not really a tractable learning since the weights (can) get updated so much in every iteration. Plus everything else is impossible to fine-tune since the lr's impact overshadows all other parameters\n",
        "\n",
        "- lr: 0.001, batch size:4, shape: [256, 256], 5 epochs, SGD\n",
        "-- Epoch: 0, Test Accuracy: 0.11124999821186066, Test Loss: 2.2863993644714355\n",
        "-- Epoch: 1, Test Accuracy: 0.2567499876022339, Test Loss: 2.2502074241638184\n",
        "-- Epoch: 2, Test Accuracy: 0.46674999594688416, Test Loss: 2.2150402069091797\n",
        "-- Epoch: 3, Test Accuracy: 0.476500004529953, Test Loss: 2.177403211593628\n",
        "-- Epoch: 4, Test Accuracy: 0.3177500069141388, Test Loss: 2.141585350036621\n",
        "-vs. evth. equal but batchsize: 8\n",
        "-- Epoch: 0, Test Accuracy: 0.18975000083446503, Test Loss: 2.2664546966552734\n",
        "-- Epoch: 1, Test Accuracy: 0.29624998569488525, Test Loss: 2.2294161319732666\n",
        "-- Epoch: 2, Test Accuracy: 0.36412501335144043, Test Loss: 2.1946041584014893\n",
        "-- Epoch: 3, Test Accuracy: 0.5042499899864197, Test Loss: 2.1515777111053467\n",
        "-- Epoch: 4, Test Accuracy: 0.5975000262260437, Test Loss: 2.106874465942383\n",
        "->  we can see that the model reaches better and more straightforward(?) results (even though we might probably need more epochs to be more precise in our claim, let's see how that looks like with an even bigger batchsize:32\n",
        "-- Epoch: 0, Test Accuracy: 0.21106229722499847, Test Loss: 2.260310173034668\n",
        "-- Epoch: 1, Test Accuracy: 0.3029153347015381, Test Loss: 2.2266860008239746\n",
        "-- Epoch: 2, Test Accuracy: 0.444888174533844, Test Loss: 2.1909801959991455\n",
        "-- Epoch: 3, Test Accuracy: 0.5446286201477051, Test Loss: 2.152503728866577\n",
        "-- Epoch: 4, Test Accuracy: 0.5709864497184753, Test Loss: 2.1096558570861816\n",
        "->  okay that does not look like as if the outcome it so much better with a bigger batch size maybe only a bit quicker in hte beginning, maybe in this case a higher learning rate would also fit better.\n"
      ],
      "metadata": {
        "id": "5TLkbYRXor-h"
      }
    }
  ]
}