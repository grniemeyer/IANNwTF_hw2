{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMidGotejDtFO7vsN2BglJl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/grniemeyer/IANNwTF_hw2/blob/main/PlayingWithMnist.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "2vDAL23ouiKA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d9c178a-4bd4-4aaa-a9fe-57c45ab4efe6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[-1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125\n",
            "  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125\n",
            "  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125\n",
            "  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125\n",
            "  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125\n",
            "  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125\n",
            "  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125\n",
            "  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125\n",
            "  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125\n",
            "  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125\n",
            "  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125\n",
            "  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125\n",
            "  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125\n",
            "  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125\n",
            "  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125\n",
            "  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125\n",
            "  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125\n",
            "  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125\n",
            "  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125\n",
            "  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125\n",
            "  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125\n",
            "  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125\n",
            "  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125\n",
            "  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125\n",
            "  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125\n",
            "  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125\n",
            "  -1.0078125  -1.0026855  -0.9923096  -1.001648   -1.0078125  -1.0078125\n",
            "  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125\n",
            "  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125\n",
            "  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125\n",
            "  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -0.9971924\n",
            "  -0.9923706  -1.0005493  -1.0078125  -1.0078125  -1.0078125  -1.0078125\n",
            "  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125\n",
            "  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125\n",
            "  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125\n",
            "  -1.0078125  -1.0078125  -1.0059204  -0.9927368  -0.9954834  -1.0060425\n",
            "  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125\n",
            "  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125\n",
            "  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0077515\n",
            "  -1.0077515  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125\n",
            "  -0.99920654 -0.9923706  -0.9975586  -1.0078125  -1.0078125  -1.0078125\n",
            "  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125\n",
            "  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125\n",
            "  -1.0078125  -1.0078125  -1.0078125  -1.0037842  -0.9951172  -1.0043945\n",
            "  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -0.99645996 -0.9923706\n",
            "  -1.0004883  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125\n",
            "  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125\n",
            "  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125\n",
            "  -1.0078125  -1.0043335  -0.9923706  -1.0005493  -1.0078125  -1.0078125\n",
            "  -1.0078125  -1.0061035  -0.99261475 -0.99316406 -1.0062866  -1.0078125\n",
            "  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125\n",
            "  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125\n",
            "  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0057373\n",
            "  -0.9923706  -1.0005493  -1.0078125  -1.0078125  -1.0078125  -1.0011597\n",
            "  -0.9923096  -0.9957886  -1.0078125  -1.0078125  -1.0078125  -1.0078125\n",
            "  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125\n",
            "  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125\n",
            "  -1.0078125  -1.0078125  -1.0078125  -1.0045776  -0.9923706  -1.0005493\n",
            "  -1.0078125  -1.0078125  -1.0078125  -0.99957275 -0.9923096  -0.9996948\n",
            "  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125\n",
            "  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125\n",
            "  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125\n",
            "  -1.0078125  -0.9996948  -0.9923096  -1.0005493  -1.0078125  -1.0078125\n",
            "  -1.0061646  -0.99316406 -0.99224854 -1.0056763  -1.0078125  -1.0078125\n",
            "  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125\n",
            "  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125\n",
            "  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0073853  -0.99346924\n",
            "  -0.9923706  -0.9951172  -0.9985962  -0.99749756 -0.99468994 -0.9923706\n",
            "  -0.99523926 -1.0076904  -1.0078125  -1.0078125  -1.0078125  -1.0078125\n",
            "  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125\n",
            "  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125\n",
            "  -1.0078125  -1.0078125  -1.0018921  -0.9923706  -0.9923706  -0.9923706\n",
            "  -0.9923096  -0.9923706  -0.9923706  -0.9923706  -1.0025635  -1.0078125\n",
            "  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125\n",
            "  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125\n",
            "  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125\n",
            "  -0.9986572  -0.9929199  -0.9989624  -1.0005493  -1.001648   -1.0028076\n",
            "  -0.9923706  -0.9923706  -1.006958   -1.0078125  -1.0078125  -1.0078125\n",
            "  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125\n",
            "  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125\n",
            "  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125\n",
            "  -1.0078125  -1.0078125  -1.0078125  -1.0026855  -0.9923096  -0.99731445\n",
            "  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125\n",
            "  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125\n",
            "  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125\n",
            "  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125\n",
            "  -1.0078125  -0.9971924  -0.9923706  -1.0005493  -1.0078125  -1.0078125\n",
            "  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125\n",
            "  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125\n",
            "  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125\n",
            "  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -0.99334717\n",
            "  -0.99243164 -1.0043945  -1.0078125  -1.0078125  -1.0078125  -1.0078125\n",
            "  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125\n",
            "  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125\n",
            "  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125\n",
            "  -1.0078125  -1.0078125  -1.0047607  -0.993103   -0.9967041  -1.0078125\n",
            "  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125\n",
            "  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125\n",
            "  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125\n",
            "  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125\n",
            "  -0.9963989  -0.9923096  -0.99261475 -1.0014038  -1.0078125  -1.0078125\n",
            "  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125\n",
            "  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125\n",
            "  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125\n",
            "  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -0.99645996 -0.9923706\n",
            "  -0.99523926 -1.0065308  -1.0078125  -1.0078125  -1.0078125  -1.0078125\n",
            "  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125\n",
            "  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125\n",
            "  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125\n",
            "  -1.0078125  -1.0078125  -0.9939575  -0.993042   -1.0058594  -1.0078125\n",
            "  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125\n",
            "  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125\n",
            "  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125\n",
            "  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125\n",
            "  -0.9934082  -0.9944458  -1.0078125  -1.0078125  -1.0078125  -1.0078125\n",
            "  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125\n",
            "  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125\n",
            "  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125\n",
            "  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125\n",
            "  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125\n",
            "  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125\n",
            "  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125\n",
            "  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125\n",
            "  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125\n",
            "  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125\n",
            "  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125\n",
            "  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125\n",
            "  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125\n",
            "  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125\n",
            "  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125  -1.0078125\n",
            "  -1.0078125  -1.0078125  -1.0078125  -1.0078125 ]], shape=(1, 784), dtype=float32) tf.Tensor([0. 0. 0. 0. 1. 0. 0. 0. 0. 0.], shape=(10,), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "### Data prep\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# tensorflow = (data) pipeline, factory\n",
        "# tensorflow tensor like array\n",
        "\n",
        "\n",
        "(train_ds,test_ds), ds_info = tfds.load('mnist', split=['train', 'test'], as_supervised=True, with_info=True)\n",
        "# print(ds_info)\n",
        "# tfds.show_examples(train_ds, ds_info)\n",
        "\n",
        "\n",
        "# remember order for exam!!\n",
        "# #ds = train_ds.map(lambda feature_dict: (feature_dict[\"image\"], feature_dict[\"label\"])) # ectract images and features\n",
        "# ds = train_ds.map(lambda image, label: (tf.reshape(image, (-1,)), label)) # flatten images, pass on labels\n",
        "# ds = ds.map(lambda image, label: ((tf.cast(image, tf.float32) / 128.0) -1, label)) # normalize images and pass on\n",
        "# ds = ds.map(lambda image, label: (image, tf.one_hot(label, depth=10))) # one hottify\n",
        "# ds = ds.shuffle(1024).batch(4) # create small buffer(9 pieces of choc bzw. 1024) in between for shuffling then batch; tf's batch operation takes care of the remainder shape\n",
        "# train_ds = ds.prefetch(4) # parralelisation: 2nd buffer that has always shuffled (prefetched) batches availabe. Shuffling & prefetching takes lot of memory\n",
        "\n",
        "\n",
        "\n",
        "# or: Combine reshaping, normalization, and one-hot encoding into a single mapping function\n",
        "def preprocess_image(image, label):\n",
        "    image = tf.reshape(image, (-1,))\n",
        "    image = (tf.cast(image, tf.float32) / 128.0) - 1\n",
        "    image = tf.expand_dims(image, axis=0)  # debugging: Add a batch dimension\n",
        "    return image, label\n",
        "\n",
        "\n",
        "# Apply the preprocessing to your datasets\n",
        "train_ds = train_ds.map(preprocess_image)\n",
        "test_ds = test_ds.map(preprocess_image)\n",
        "\n",
        "train_ds = train_ds.map(lambda image, label: ((tf.cast(image, tf.float32) / 128.0) - 1, tf.one_hot(label, depth=10)))\n",
        "test_ds = test_ds.map(lambda image, label: ((tf.cast(image, tf.float32) / 128.0) - 1, tf.one_hot(label, depth=10)))\n",
        "\n",
        "\n",
        "\n",
        "for x, y in train_ds.take(1):\n",
        "  print(x, y)\n",
        "\n",
        "# • How many training/test images are there?\n",
        "# - 'test' 10,000, train' 60,000\n",
        "# • What’s the image shape?\n",
        "# - (28, 28, 1)\n",
        "# • What range are pixel values in?\n",
        "# - In grayscale images, a pixel value of 0 represents black, and 255 represents white"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### 2.3 Building a DNN with TF\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "class MyDeepNN(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    #self.image = image\n",
        "    super(MyDeepNN, self).__init__()\n",
        "    self.dense1 = tf.keras.layers.Dense(256, activation=tf.nn.relu)\n",
        "    self.dense2 = tf.keras.layers.Dense(256, activation=tf.nn.relu)\n",
        "    self.out = tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    x = self.dense1(inputs)\n",
        "    x = self.dense2(x)\n",
        "    y = self.out(x)\n",
        "    return y\n",
        "\n",
        "\n",
        "# class MyDeepNN(tf.keras.Model):\n",
        "#     def __init__(self):\n",
        "#         super(MyDeepNN, self).__init__()\n",
        "#         self.dense1 = Dense(256, activation=tf.nn.relu)\n",
        "#         self.dense2 = Dense(256, activation=tf.nn.relu)\n",
        "#         self.out = Dense(10, activation=tf.nn.softmax)\n",
        "\n",
        "#     def call(self, inputs):\n",
        "#         x = self.dense1(inputs)\n",
        "#         x = self.dense2(x)\n",
        "#         y = self.out(x)\n",
        "#         return y\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "f9CP2B7hFHRP"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### 2.4 Training the Network\n",
        "\n",
        "\n",
        "# define hyperparameters\n",
        "nr_epochs = 10\n",
        "learning_rate = 0.1\n",
        "\n",
        "# initialize stuff\n",
        "mymodel = MyDeepNN()\n",
        "cross_entropy_loss = tf.keras.losses.CategoricalCrossentropy()\n",
        "opti = tf.keras.optimizers.SGD(learning_rate)\n",
        "\n",
        "# lists (for viualization later)\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "test_accuracies = []\n",
        "\n",
        "\n",
        "\n",
        "def test(model, test_ds, loss_function):\n",
        "  test_accuracy_aggregator = []\n",
        "  test_loss_aggregator = []\n",
        "\n",
        "  for (image, target) in test_ds.take(5): # adjusted to 5 for now\n",
        "    prediction = model(image)\n",
        "    # debugging 5.0: Assuming target is one-hot encoded, use tf.reduce_sum along the axis to get the correct shape\n",
        "    target = tf.reduce_sum(target, axis=0)\n",
        "    prediction = tf.squeeze(prediction, axis=0)  # debugging 2.0: Remove the batch dimension\n",
        "    # Print the shapes for debugging\n",
        "    print(\"Target shape:\", target.shape)\n",
        "    print(\"Prediction shape:\", prediction.shape)\n",
        "    # debugging 6.0: Reshape the target to match the shape of prediction\n",
        "    target = tf.reshape(target, prediction.shape)\n",
        "    sample_test_loss = loss_function(target, prediction)\n",
        "    # # debugging 3.0: Check the dimensions before applying np.argmax\n",
        "    # if len(target.shape) == 1:\n",
        "    #   sample_test_accuracy = np.argmax(target) == np.argmax(prediction)\n",
        "    # else:\n",
        "    #   sample_test_accuracy = np.argmax(target, axis=1) == np.argmax(prediction, axis=1)\n",
        "\n",
        "    sample_test_accuracy = np.argmax(target, axis=1) == np.argmax(prediction, axis=1)\n",
        "    sample_test_accuracy = np.mean(sample_test_accuracy)\n",
        "    test_loss_aggregator.append(sample_test_loss.numpy())\n",
        "    test_accuracy_aggregator.append(np.mean(sample_test_accuracy))# double averaging??\n",
        "\n",
        "  test_loss = tf.reduce_mean(test_loss_aggregator)\n",
        "  test_accuracy = tf.reduce_mean(test_accuracy_aggregator)\n",
        "\n",
        "  return test_loss, test_accuracy\n",
        "\n",
        "\n",
        "# test before sending through NN, for comparison afterwards\n",
        "test_loss, test_accuracy = test(mymodel, test_ds, cross_entropy_loss)\n",
        "test_losses.append(test_loss)\n",
        "test_accuracies.append(test_accuracy)\n",
        "\n",
        "# test with train_ds before training NN\n",
        "train_loss, _ = test(mymodel, train_ds, cross_entropy_loss)\n",
        "train_losses.append(train_loss)\n",
        "\n",
        "\n",
        "# tests on 1 input-target pair\n",
        "def train_step(model, input_data, target, loss_function, optimizer):\n",
        "  with tf.GradientTape() as tape:\n",
        "    prediction = model(input_data)\n",
        "    loss = loss_function(target, prediction)\n",
        "  gradients = tape.gradient(loss, model.trainable_variables) # trainable_variables = weights?\n",
        "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "  return loss\n",
        "\n",
        "\n",
        "\n",
        "for epoch in range(nr_epochs):\n",
        "#  print(f\"epoch: {str(epoch)} with accuracy {test_accuracies[-1]}\") # starting with last element in test_accuracies\n",
        "\n",
        "  # training\n",
        "  epoch_losses = []\n",
        "  for input, label in train_ds:\n",
        "    train_loss = train_step(mymodel, input, label, cross_entropy_loss, opti)\n",
        "    epoch_losses.append(train_loss)\n",
        "\n",
        "  # track training loss\n",
        "  train_losses.append(tf.reduce_mean(epoch_losses)) #take the mean\n",
        "\n",
        "  # testing (track test loss and test accuracy)\n",
        "  test_loss, test_accuracy = test(mymodel,test_ds, cross_entropy_loss)\n",
        "  test_losses.append(test_loss)\n",
        "  test_accuracies.append(test_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "id": "9ETX31V2e6mp",
        "outputId": "0669bfcc-0500-4ba9-f2b7-a938bd9c4040"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Target shape: ()\n",
            "Prediction shape: (10,)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-61c503480db2>\u001b[0m in \u001b[0;36m<cell line: 53>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;31m# test before sending through NN, for comparison afterwards\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmymodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0mtest_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0mtest_accuracies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_accuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-61c503480db2>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(model, test_ds, loss_function)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Prediction shape:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;31m# debugging 6.0: Reshape the target to match the shape of prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0msample_test_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;31m# # debugging 3.0: Check the dimensions before applying np.argmax\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     ]\n\u001b[0;32m---> 60\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     61\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__Reshape_device_/job:localhost/replica:0/task:0/device:CPU:0}} Input to reshape is a tensor with 1 values, but the requested shape has 10 [Op:Reshape]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training\n",
        "\n",
        "EPOCHS = 10\n",
        "\n",
        "model = MLP_Model(layer_sizes=[256, 256])\n",
        "cce = tf.keras.CategoricalCrossEntropy()\n",
        "optimizer = tf.keras.optimizers.legacy.SGD() # stochastic gradient as optimizer\n",
        "ds = ds # does nothing, just as reminder from above\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  losses = []\n",
        "\n",
        "  for x, target in ds:\n",
        "    with tf.GradientTape() as tape: # context manager\n",
        "      pred = model(x)\n",
        "      loss = cce(target, pred)\n",
        "    gradients = tape.gradient(loss, model.varibles) # calculate gradient outside of GradientTape, otherwis it calculates gradient of gradient (memory!!, rare exceptions)\n",
        "    optimizer.appy_gradients(zip(gradients, model.variables))\n",
        "    losses.append(loss.numpy())\n",
        "  print(np.mean(losses))"
      ],
      "metadata": {
        "id": "zD6Q6lWMFsaf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cidN_XRQHEri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wSkE6uDJHEP4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Creation via Subclassing from tf.keras.Model\n",
        "\n",
        "# #inherit from keras.Model\n",
        "# class MLP_Model(self, layer_size):\n",
        "#   def __init__(self, layer_size, output_size=10):\n",
        "#     super().__init__()\n",
        "#     self.layers = []\n",
        "#     #layer_size e.g.[256, 256]\n",
        "\n",
        "#     for layer_size in layer_sizes:\n",
        "#       new_layer = tf.keras.layers.Dense(units=layer_size, activation=\"sigmoid\")\n",
        "#       self.layers.append(new_layer)\n",
        "#     self.output_layer = tf.keras.layers.Dense(units=output_size, activation=\"softmax\") # output layer ist deshalb specified seperately bc we need different activation function\n",
        "\n",
        "#   def call(self, x):\n",
        "#     for layer in self.layers:\n",
        "#       x = layer(x) # call layer on input\n",
        "#     y = self.output.layer(x)\n",
        "#     return y"
      ],
      "metadata": {
        "id": "tK6rhidICYz5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "outputId": "95c47402-1f09-4143-ee8d-ca19e13b73d7"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-8c8c162bb783>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Model Creation via Subclassing from tf.keras.Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#inherit from keras.Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tf'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pmP01ImfBLkn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}